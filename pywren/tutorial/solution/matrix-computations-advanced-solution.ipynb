{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Large Scale Matrix Computations\n",
    "\n",
    "In this notebookwe will walk through some of the more advanced things you can achieve with PyWren. Namely using S3 as a backing store we will implement a nearest neighbor classifier algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "import boto3\n",
    "import cloudpickle\n",
    "import itertools\n",
    "import concurrent.futures as fs\n",
    "import io\n",
    "import numpy as np\n",
    "import time\n",
    "from importlib import reload\n",
    "from sklearn import metrics\n",
    "import pywren\n",
    "import pywren.wrenconfig as wc\n",
    "import itertools\n",
    "from operator import itemgetter\n",
    "import sys\n",
    "sys.path.insert(0,\"..\")\n",
    "import matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_BUCKET = wc.default()['s3']['bucket']\n",
    "DEFAULT_PREFIX = wc.default()['s3']['pywren_prefix']\n",
    "pwex = pywren.default_executor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Matrix Multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One nice thing about PyWren is it allows users to integrate existing python libraries easily.\n",
    "For the following exercise, we are going to use some popular python libraries, e.g., NumPy, to work on some matrix multiplication problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def my_function(b):\n",
    "    x = np.random.normal(0, b, 1024)\n",
    "    A = np.random.normal(0, b, (1024, 1024))\n",
    "    return np.dot(A, x)\n",
    "\n",
    "res = pwex.map(my_function, np.linspace(0.1, 10, 100))\n",
    "pywren.wait(res)\n",
    "[f.result() for f in res]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Distributed Nearest Neighbor with Large Scale Matrix Multiplication \n",
    "\n",
    "A problem with the above method is that, we are limited to working with \"small\" matrices, that fit in the memory of a single lambda instance. With a little work we can write a \"ShardedMatrix\" wrapper that shards numpy matrices across S3 objects (the source code for this can be found in matrix.py) This allows us to use PyWren's map functionality to access different parts of the matrix. We can further use this functionality to compute a large scale matrix multiplication.\n",
    "\n",
    "\n",
    "In the below example we will implement a distributed nearest neighbor implementation on top of PyWren and this ShardedMatrix abstraction. Note that nearest neighbor is often a hard to implement algorithm on BSP systems such as Apache Spark due to a high communication cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_mldata\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets download the mnist dataset. This cell should take about 3 minutes to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = fetch_mldata('MNIST original', data_home=\"/tmp/\")['data'].astype('float32')\n",
    "y = fetch_mldata('MNIST original', data_home=\"/tmp/\")['target']\n",
    "X_train = X[:60000, :]\n",
    "y_train = y[:60000, np.newaxis]\n",
    "\n",
    "X_test = X[60000:, :]\n",
    "y_test = y[60000:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now \"shard\" the mnist matrix with a shard_size of 4000, what this means is that, we will convert the 60000 by 784 matrix into 30 separate 4000 x 784 numpy matrices that will be split across different S3 Keys. The first argument is the S3 folder where these submatrices can be found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time X_train_sharded = matrix.ShardedMatrix(\"x_train\", shape=X_train.shape, prefix=DEFAULT_PREFIX, bucket=DEFAULT_BUCKET, shard_sizes=[4000,784])\n",
    "%time X_train_sharded.shard_matrix(X_train, n_jobs=16)\n",
    "\n",
    "%time X_test_sharded = matrix.ShardedMatrix(\"x_test\", shape=X_test.shape, prefix=DEFAULT_PREFIX, bucket=DEFAULT_BUCKET, shard_sizes=[4000,784])\n",
    "%time X_test_sharded.shard_matrix(X_test, n_jobs=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our sharded matrices we can compute a local nearest neighbor classifier and compare it with one we will compute with PyWren. If we do everything correctly the PyWren implementation should be identical to the local one, but with a better scaling with dataset size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_local_nearest_neighbor_labels(X_train, X_test, y_test, y_train):\n",
    "    # compute a distance matrix\n",
    "    train_norms = np.linalg.norm(X_train, axis=1)[:, np.newaxis] ** 2\n",
    "    test_norms = np.linalg.norm(X_test, axis=1)[np.newaxis, :] ** 2\n",
    "    return y_train[np.argmin(train_norms + -2*X_train.dot(X_test.T)+ test_norms, axis=0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time y_test_pred = compute_local_nearest_neighbor_labels(X_train, X_test, y_test, y_train)\n",
    "print(\"Accuracy is \", metrics.accuracy_score(y_test_pred, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lets try using PyWren.\n",
    "Our strategy will be to use PyWren to map over (training point, testing point) pairs (in a blockwise fashion), and generate the distance matrix in a sharded form. Then we can launch another PyWren job to extract the nearest neighbors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create an \"empty\" ShardedMatrix on S3 (we will fill this Matrix in with pywren)\n",
    "D_sharded = matrix.ShardedMatrix(\"D\", shape=(X_train.shape[0], X_test.shape[0]), shard_sizes=[4000,4000], prefix=DEFAULT_PREFIX, bucket=DEFAULT_BUCKET)\n",
    "\n",
    "def compute_pywren_nearest_neighbor_distance_matrix(block_pair, X_train_sharded, X_test_sharded, D_sharded):\n",
    "    block0,block1 = block_pair\n",
    "    # compute a distance matrix block\n",
    "    X_train_block = X_train_sharded.get_block(block0, 0)\n",
    "    X_test_block = X_test_sharded.get_block(block1, 0)\n",
    "    train_norms = np.linalg.norm(X_train_block, axis=1)[:, np.newaxis] ** 2\n",
    "    test_norms = np.linalg.norm(X_test_block, axis=1)[np.newaxis, :] ** 2\n",
    "    D_block = train_norms + -2*X_train_block.dot(X_test_block.T)+ test_norms\n",
    "    D_sharded.put_block(block0, block1, D_block)\n",
    "    return 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pywren_distance_function = lambda bidx: compute_pywren_nearest_neighbor_distance_matrix(bidx, X_train_sharded, X_test_sharded, D_sharded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that our function works locally\n",
    "pywren_distance_function((0,0)) # compute the first block of distance matrix\n",
    "D_sharded.get_block(0,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time futures = pwex.map(pywren_distance_function, D_sharded.block_idxs)\n",
    "%time pywren.wait(futures)\n",
    "[f.result() for f in futures] # make sure nothing exceptioned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_argmin(block_pair, D_sharded):\n",
    "    D_block = D_sharded.get_block(*block_pair)\n",
    "    offset = block_pair[0]*D_sharded.shard_sizes[0] \n",
    "    return (block_pair[1], offset + np.argmin(D_block, axis=0), np.min(D_block, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time futures = pwex.map(lambda x: find_argmin(x, D_sharded), sorted(D_sharded.block_idxs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time pywren.wait(futures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results = [f.result() for f in futures]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_results_from_pywren(results):\n",
    "    mins = []\n",
    "    for _, group in itertools.groupby(sorted(results, key=itemgetter(0)), key=itemgetter(0)):\n",
    "        group = list(group)\n",
    "        argmins = np.vstack([g[1] for g in group])\n",
    "        argminmin = np.argmin(np.vstack([g[2] for g in group]), axis=0)\n",
    "        mins.append(argmins[argminmin, np.arange(argmins.shape[1])])\n",
    "    return np.hstack(mins)\n",
    "y_test_pred_pywren = y_train[compute_results_from_pywren(results)]\n",
    "print(\"Accuracy is \", metrics.accuracy_score(y_test, y_test_pred_pywren))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The advantage of this PyWren implementation is that we are executing in parallel over the entire matrix. So as we get more training and test points our implementation will scale gracefully (and dynamically) along with it. The above method can scale comfortably up to 1 million training points and 1 million test points with no code changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
