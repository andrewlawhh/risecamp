{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL Exercise 3 - Deep Q Networks\n",
    "\n",
    "**GOAL:** The goal of this exercise is to demonstrate how to use the deep Q networks (DQN) algorithm and compare its efficiency with PPO. You will also learn how to use RLlib's command-line benchmark API and visualize results with TensorBoard.\n",
    "\n",
    "To understand how to use **RLlib**, see the documentation at http://rllib.io.\n",
    "\n",
    "DQN was *the* first deep RL algorithm, and is described in detail in https://arxiv.org/abs/1312.5602.\n",
    "\n",
    "In DQN, instead of training a policy network to directly emit output actions from the observation, we learn a Q function that models the expected outcome of taking certain actions. This model is then used to compute the optimal actions at each step.\n",
    "\n",
    "Unlike policy gradient algorithms such as PPO, DQN can learn from past experiences through *experience replay*. This allows DQN to use experiences multiple times over the course of training, improving its sample efficiency. In this exercise we are going to use a single-process configuration for DQN, but RLlib does provide a distributed variant of DQN: https://ray.readthedocs.io/en/latest/rllib-algorithms.html#distributed-prioritized-experience-replay-ape-x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running DQN with the command-line API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time, we won't use the Python API. For well-known benchmark environments such as CartPole-v0, it is more convenient to run them from the command line.\n",
    "\n",
    "**EXERCISE**: Open a new terminal in Jupyter lab using the \"+\" button, and run:\n",
    "\n",
    "`$ rllib train --run=DQN --env=CartPole-v0`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison with PPO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXERCISE**: Compare the performance of DQN with PPO. How many timesteps does it take to reach a reward of 150?\n",
    "\n",
    "Note that you can run PPO from the command line as well. Configuration can be passed via the --config flag.\n",
    "\n",
    "`$ rllib train --run=PPO --env=CartPole-v0 --config='{\"num_sgd_iter\": 30}'`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize results with TensorBoard\n",
    "\n",
    "**EXERCISE**: Finally, you can visualize your training results using TensorBoard. To do this, run:\n",
    "    \n",
    "`$ tensorboard --logdir=~/ray_results`\n",
    "\n",
    "And open your browser to the address printed. Compare the learning curves of PPO vs DQN. Toggle the horizontal axis between both the \"STEPS\" and \"RELATIVE\" view to compare efficiency in number of timesteps vs real time time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
