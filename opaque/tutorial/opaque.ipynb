{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Opaque Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Opaque is a package for Apache Spark SQL that enables analytics on sensitive data in an untrusted cloud. Opaque achieves this using Intel SGX trusted hardware enclaves, which make it possible to operate on encrypted data without revealing it to an attacker -- even one that controls the OS or hypervisor. Queries to Opaque are issued from a trusted client such as your laptop, which holds the encryption keys and verifies the trusted hardware.\n",
    "\n",
    "Opaque queries are issued using Spark SQL's DataFrame API. Opaque supports a limited but growing subset of this API, allowing supported queries to be run securely with just a few code changes.\n",
    "\n",
    "In this tutorial we will set up Opaque, learn to write supported queries over some sample data, and verify that Opaque is encrypting the data by inspecting the contents in memory.\n",
    "\n",
    "For the purpose of the tutorial, we are running Spark in local mode, meaning the driver and the workers run in the same process and the SGX enclaves are only simulated. For a real deployment, the workers would run on a cluster with real SGX hardware.\n",
    "\n",
    "This tutorial is written as a Jupyter notebook. You can execute a cell using `Shift+Enter`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up Apache Spark and Opaque"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Jupyter notebook provides a Scala shell. To use Spark and Opaque, we need to import them. Ordinarily this would require specifying them as dependencies before launching the shell, but the notebook provides special syntax for loading the dependencies at runtime from the Ivy package manager:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import $ivy.`org.apache.spark::spark-sql:2.0.2`\n",
    "import $ivy.`edu.berkeley.cs.amplab::opaque:0.1`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to start a [`SparkSession`](https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-SparkSession.html), which is the entry point to Spark SQL. The call to `setJars()` passes in the Opaque dependency, which is already built for you, so that if we were running on a cluster Spark could launch Opaque on the workers as well. The call to `master()` specifies that we are running in the current process with 1 worker thread."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark._\n",
    "import org.apache.spark.sql._\n",
    "import org.apache.log4j._\n",
    "\n",
    "LogManager.getLogger(\"org\").setLevel(Level.WARN)\n",
    "\n",
    "val spark = SparkSession.builder()\n",
    "  .config(new SparkConf().setJars(Seq(\"opaque/target/scala-2.11/opaque_2.11-0.1.jar\")))\n",
    "  .appName(\"notebook\")\n",
    "  .master(\"local[1]\")\n",
    "  .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we import some useful types from Spark and Opaque, and inject Opaque's extensions into Spark SQL using `initSQLContext()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.catalyst.analysis._\n",
    "import org.apache.spark.sql.catalyst.dsl._\n",
    "import org.apache.spark.sql.catalyst.errors._\n",
    "import org.apache.spark.sql.catalyst.expressions._\n",
    "import org.apache.spark.sql.catalyst.plans.logical._\n",
    "import org.apache.spark.sql.catalyst.rules._\n",
    "import org.apache.spark.sql.catalyst.util._\n",
    "import org.apache.spark.sql.execution\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.types._\n",
    "\n",
    "import spark.implicits._\n",
    "\n",
    "import edu.berkeley.cs.rise.opaque.implicits._\n",
    "\n",
    "edu.berkeley.cs.rise.opaque.Utils.initSQLContext(spark.sqlContext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating an encrypted DataFrame\n",
    "\n",
    "Now we can create our first DataFrame. At first, we'll just specify its contents inline. First we create `data`, a list of three tuples. Then we load those into `df`, a Spark SQL DataFrame. Finally, we use Opaque's new `.encrypted()` method on DataFrames to encrypt it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val data = Seq((\"foo\", 4), (\"bar\", 1), (\"baz\", 5))\n",
    "val df = spark.createDataFrame(data).toDF(\"word\", \"count\")\n",
    "val dfEncrypted = df.encrypted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see how Opaque would execute a simple query such as a filter over this DataFrame to drop `(\"bar\", 1)`. Spark SQL's `.explain()` method on DataFrames allows us to do this. The `true` argument gives extended output.\n",
    "\n",
    "The output of `.explain(true)` has four parts:\n",
    "1. The Parsed Logical Plan shows a representation of the query as we entered it. The `LocalRelation` operator represents `data`, the `Project` operator represents naming each column, the `Encrypt` operator is from our call to `df.encrypted`, and the `Filter` operator is from our call to `dfEncrypted.filter(...)`.\n",
    "2. The Analyzed Logical Plan shows the query after analysis, which binds the `count` column to a unique identifier present in the input relation.\n",
    "3. The Optimized Logical Plan shows the query after Opaque's rules have been applied. Now all the operator names start with `Encrypted`, showing that they are Opaque operators that will run inside SGX enclaves.\n",
    "4. The Physical Plan shows the physical operators selected for each logical operator. For such a simple query, there is a one-to-one correspondence between the two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfEncrypted.filter($\"count\" > lit(3)).explain(true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark's DataFrame API is lazy, so the query hasn't actually run yet. Let's run it and see the results using `.show()`. You should see a pretty-printed table with only two out of the three tuples.\n",
    "\n",
    "Somewhere above it, the message `Starting an enclave` should be printed. This is confirmation that Opaque is working!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfEncrypted.filter($\"count\" > lit(3)).show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the raw contents of these DataFrames. Since `df` is unencrypted, we should be able to see its contents in plain text as Spark SQL `Row` objects. On the other hand, `dfEncrypted` should contain an encrypted binary representation of the rows.\n",
    "\n",
    "Since `dfEncrypted` does not expose its contents as an unencrypted RDD, we have to use the developer API to access them. We have provided a function called `rawContents` to do this. This function in turn uses a helper function called `bytesToString`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rdd.collect()\n",
    "\n",
    "/** Convert `bytes` to a truncated hex string. */\n",
    "def bytesToString(bytes: Array[Byte]): String = {\n",
    "    val limit = 200\n",
    "    val (truncBytes, ellipsis) = if (bytes.length < limit) {\n",
    "        (bytes, \"\")\n",
    "    } else {\n",
    "        (bytes.take(limit), \"...\")\n",
    "    }\n",
    "    truncBytes.map(b => \"%02X\".format(b)).mkString(\n",
    "        \"%d bytes: [\".format(bytes.length), \" \", ellipsis + \"]\\n\")\n",
    "}\n",
    "\n",
    "/**\n",
    " * Given `encDF`, an encrypted DataFrame, extract a string representation\n",
    " * of each of its encrypted blocks.\n",
    " */\n",
    "def rawContents(encDF: DataFrame): Array[String] = {\n",
    "  import edu.berkeley.cs.rise.opaque.execution.OpaqueOperatorExec\n",
    "  val blocks = encDF.queryExecution.executedPlan.asInstanceOf[OpaqueOperatorExec].executeBlocked().collect()\n",
    "  for (b <- blocks) yield bytesToString(b.bytes)\n",
    "}\n",
    "\n",
    "rawContents(dfEncrypted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Next we will write some Opaque queries on larger datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing Opaque queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be working with some synthetic medical datasets located in the `opaque/data/disease/` directory:\n",
    "\n",
    "- `patient-125.csv` contains 125 patient records with patient ID, the ID of the patient's disease, and the patient's name. This is sensitive data and must always be encrypted when we are working with it.\n",
    "- `disease.csv` contains over 70,000 known diseases. Each disease record contains a disease ID, the ID of the group of genes responsible for the disease, and the name of the disease.\n",
    "- `treatment.csv` contains about 140,000 potential treatments. Each treatment record contains a treatment ID, the ID of the disease it treats, the name of the treatment, and how much it costs.\n",
    "\n",
    "First, let's load these datasets and encrypt them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val patientDF = spark.read.schema(\n",
    "  StructType(Seq(\n",
    "    StructField(\"p_id\", IntegerType),\n",
    "    StructField(\"p_disease_id\", StringType),\n",
    "    StructField(\"p_name\", StringType))))\n",
    "  .csv(s\"opaque/data/disease/patient-125.csv\")\n",
    "  .encrypted\n",
    "\n",
    "val diseaseDF = spark.read.schema(\n",
    "  StructType(Seq(\n",
    "    StructField(\"d_disease_id\", StringType),\n",
    "    StructField(\"d_gene_id\", IntegerType),\n",
    "    StructField(\"d_name\", StringType))))\n",
    "  .csv(s\"opaque/data/disease/disease.csv\")\n",
    "  .encrypted\n",
    "\n",
    "val treatmentDF = spark.read.schema(\n",
    "  StructType(Seq(\n",
    "    StructField(\"t_id\", IntegerType),\n",
    "    StructField(\"t_disease_id\", StringType),\n",
    "    StructField(\"t_name\", StringType),\n",
    "    StructField(\"t_cost\", IntegerType))))\n",
    "  .csv(s\"opaque/data/disease/treatment.csv\")\n",
    "  .encrypted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Now we can get a preview of their contents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// TODO: Inspect the contents of the three DataFrames using show()."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Let's also verify that they contain the expected number of rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// TODO: Count the number of rows in each DataFrame (patientDF, diseaseDF, treatmentDF) using count()."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Now let's run some analytics. For each patient, we want to know the name of the disease they have, not just its ID.\n",
    "\n",
    "<details>\n",
    "<summary>Hint 1: Overview</summary>\n",
    "    The patient names and the disease names are in different DataFrames: <code>patientDF</code> and <code>diseaseDF</code>. To bring them together, you'll need to <em>join</em> the two DataFrames together, then <em>select</em> out the columns we're interested in.\n",
    "</details>\n",
    "<details>\n",
    "<summary>Hint 2: DataFrame syntax</summary>\n",
    "The syntax to join two DataFrames is <code>dfA.join(dfB, $\"column_in_dfA\" === $\"column_in_dfB\")</code>.\n",
    "    \n",
    "The syntax to select columns from a DataFrame is <code>df.select($\"column1\", $\"column2\")</code>.\n",
    "</details>\n",
    "<details>\n",
    "<summary>Hint 3: Column names</summary>\n",
    "    You'll want to join <code>patientDF</code>'s <code>p_disease_id</code> column with <code>diseaseDF</code>'s <code>d_disease_id</code> column. Then you can select out the <code>p_name</code> and <code>d_name</code> columns.\n",
    "</details>\n",
    "<details>\n",
    "<summary>Answer</summary>\n",
    "    <code>patientDF.join(diseaseDF, $\"p_disease_id\" === $\"d_disease_id\").select($\"p_name\", $\"d_name\").show()</code>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// TODO: Join patientDF and diseaseDF to correlate patient names with disease names."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** How expensive is each disease to treat? For each disease, let's find the price of the lowest-cost treatment. (No need to find the treatment ID, just its cost.)\n",
    "\n",
    "<details>\n",
    "<summary>Hint 1: Overview</summary>\n",
    "    Remember that each treatment in <code>treatmentDF</code> treats a particular disease, identified by disease id. We want to <em>group</em> treatments by the disease they treat, then perform an <em>aggregation</em> within each group to find the <em>minimum</em>-cost treatment.\n",
    "</details>\n",
    "<details>\n",
    "<summary>Hint 2: DataFrame syntax</summary>\n",
    "The syntax to group a DataFrame by a particular column is <code>df.groupBy($\"column\")</code>.\n",
    "    \n",
    "The syntax to aggregate a grouped DataFrame to find the minimum value for each group is <code>df.groupBy($\"column1\").agg(min($\"column2\"))</code>.\n",
    "</details>\n",
    "<details>\n",
    "<summary>Answer</summary>\n",
    "    <code>treatmentDF.groupBy($\"t_disease_id\").agg(min(\"t_cost\").as(\"t_min_cost\")).show()</code>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// TODO: Run an aggregation over treatmentDF to find the cost of the cheapest treatment for each disease."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Now let's put all three datasets together. For each patient, what disease do they have, and what is the lowest cost to treat it?\n",
    "\n",
    "<details>\n",
    "<summary>Hint 1: Overview</summary>\n",
    "    Since this query involves correlating data from all three DataFrames, we'll need to <em>join</em> all of them. The common key that they all share is the <em>disease id</em>. Then we can <em>select</em> out the three columns we're interested in: patient name, disease name, and minimum treatment cost.\n",
    "</details>\n",
    "<details>\n",
    "<summary>Hint 2: DataFrame syntax</summary>\n",
    "To join three DataFrames together, we have to join them in pairs: <code>dfA.join(dfB.join(dfC, $\"b_join_id\" === $\"c_join_id\"), $\"a_join_id\" === $\"b_join_id\")</code>.\n",
    "</details>\n",
    "<details>\n",
    "<summary>Answer</summary>\n",
    "    <pre>val minCostTreatments = treatmentDF.groupBy($\"t_disease_id\").agg(min(\"t_cost\").as(\"t_min_cost\"))\n",
    "minCostTreatments.join(\n",
    "  diseaseDF.join(\n",
    "    patientDF,\n",
    "    $\"d_disease_id\" === $\"p_disease_id\"),\n",
    "  $\"d_disease_id\" === $\"t_disease_id\")\n",
    "  .select($\"p_name\", $\"d_name\", $\"t_min_cost\")\n",
    "  .show()</pre>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// TODO: Join patientDF, diseaseDF, and the grouped version of treatmentDF from above to generate this report."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
