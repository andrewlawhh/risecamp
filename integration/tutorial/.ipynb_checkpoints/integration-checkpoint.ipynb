{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RISE Camp Capstone Exercise\n",
    "\n",
    "In this exercise, you will see how many of the projects you've learned about in the last couple days fit together. Those of you who attended last year's RISE Camp will remember the Pong integration exercise that trained an RL policy in Ray and deployed it in Clipper. Today, we're going to extend that verison by tracking some experiments in Flor and encrypting our models with Wave.\n",
    "\n",
    "We will train models to play Pong. The first two will use [imitation learning](https://blog.statsbot.co/introduction-to-imitation-learning-32334c3b1e7a) to learn how to play, and the third will train a reinforcement learning policy using RLlib and Ray. Flor will track the training processes for all three models. We will also encrypt each one of these models with WAVE and deploy & serve the models in Clipper.\n",
    "\n",
    "For those of you unfamiliar with imitation learning, we will simply take the state of the game (location of the ball, location of the paddle, etc.) combined with the labeled action of a human player and train a classifier that responds ot the state of the game board with the action to take.\n",
    "\n",
    "Finally, you'll play a game (or more!) against each of the three models. We'll aggregate the results to see which agent performs the best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python compatibility imports\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import gym\n",
    "import pong_py\n",
    "import cloudpickle\n",
    "\n",
    "# ray imports\n",
    "import ray\n",
    "from ray.tune.registry import register_env\n",
    "from ray.rllib.agents import ppo\n",
    "\n",
    "import flor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set Flor metadata for the notebook\n",
    "flor.setNotebookName('integration.ipynb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a separate notebook, we have set up a WAVE client and defined some helper functions that we'll use below. Feel free to look at the `wave-setup.ipynb` file in this directory if you'd like to dig in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run wave-setup.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call Wave helper function to create granting and receiving entities\n",
    "orgEntity, recipientEntity = createWaveEntities()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imitation Learning\n",
    "\n",
    "## Model Training\n",
    "\n",
    "First, we're going to define three functions---`preproc_imitation`, `train_imitation_model`, and `encrypt_model`, which clean the input data, train an imitation learning model, and encrypt that model using Wave, respectively. \n",
    "\n",
    "The preprocessing function reads an input CSV and converts the `up`, `down`, and `stay` labels into numerical values. It also normalizes the all the numerical values (the location of the controlled paddle, the location & velocity of the ball, and the previous location of the ball)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PRE-DEFINED FLOR FUNCTION. PLEASE DO NOT CHANGE.\n",
    "\n",
    "@flor.func\n",
    "def preproc_imitation(imitation_data, procd_imitation_data, **kwargs):\n",
    "    import pandas as pd\n",
    "    df_data = pd.read_csv(imitation_data)\n",
    "    df_data.columns = [\"label\", \"paddle_y\", \"ball_x\", \"ball_y\", \"ball_dx\", \"ball_dy\", \"x_prev\", \"y_prev\", \"user\"]\n",
    "    \n",
    "    # drop the user column because we don't want to train on it\n",
    "    df_data = df_data.drop(labels=\"user\", axis=1)\n",
    "\n",
    "    # discretize the labels\n",
    "    def convert_label(label):\n",
    "        \"\"\"Convert labels into numeric values\"\"\"\n",
    "        if(label==\"down\"):\n",
    "            return 1\n",
    "        elif(label==\"up\"):\n",
    "            return 2\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    df_data['label'] = df_data['label'].apply(convert_label)\n",
    "    df_data.loc[:, \"paddle_y\":\"y_prev\"] = df_data.loc[:, \"paddle_y\":\"y_prev\"]/500.0\n",
    "    df_data.to_json(procd_imitation_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model training function takes a JSON blob of the cleaned data and fits a SciKit Learn logistic regression model to classify the action to take based on the input features. The model is pickled and dumped into a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PRE-DEFINED FLOR FUNCTION. PLEASE DO NOT CHANGE.\n",
    "\n",
    "@flor.func\n",
    "def train_imitation_model(procd_imitation_data, model, **kwargs):\n",
    "    import cloudpickle\n",
    "    import pandas as pd\n",
    "    from sklearn import linear_model\n",
    "    df_data = pd.read_json(procd_imitation_data)\n",
    "    \n",
    "    labels = df_data['label']\n",
    "    training_data= df_data.drop(['label'], axis=1)\n",
    "\n",
    "    skmodel = linear_model.LogisticRegression()\n",
    "    skmodel.fit(training_data, labels)\n",
    "    with open(model, 'wb') as f:\n",
    "        cloudpickle.dump(skmodel, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the `encrypt_model` function takes the model we trained above and a handle to a WAVE entity that has access to all models. It uses the WAVE entity to encrypt the model and serializes the ciphered model into a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PRE-DEFINED FLOR FUNCTION. PLEASE DO NOT CHANGE.\n",
    "\n",
    "@flor.func\n",
    "def encrypt_model(granting_entity, model, model_tag, encrypted_model, **kwargs):\n",
    "    import wave3 as wv\n",
    "    granting_entity = deserializeEntity(granting_entity)\n",
    "    \n",
    "    # read the model binary, so we can encrypt it\n",
    "    with open(model, 'rb') as f:\n",
    "        model = f.read()\n",
    "    \n",
    "    # NOTE: We are relying on a global handle to WAVE here. \n",
    "    # In practice, we would have to recreate this handle explicitly.\n",
    "    encrypt_response = wave.EncryptMessage(\n",
    "        wv.EncryptMessageParams(\n",
    "            # the namespace is the organization\n",
    "            namespace=granting_entity.hash,\n",
    "            resource=\"models/pong/\" + model_tag,\n",
    "            content=model))\n",
    "    \n",
    "    with open(encrypted_model, 'wb') as f:\n",
    "        f.write(encrypt_response.ciphertext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define a Flor experiment called `pong-imitation` and link together the input data and the functions defined above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHANGE ME LATER\n",
    "DATA_FILE = 'imitation-small.csv'\n",
    "\n",
    "# get the small or large tag from the DATA_FILE variable\n",
    "model_tag = DATA_FILE.split('.')[0].split('-')[1]\n",
    "\n",
    "ENTITY_FILE = 'org_entity.bin'\n",
    "\n",
    "with flor.Experiment('pong-imitation') as ex:\n",
    "    # load data into an artifact\n",
    "    imitation_data = ex.artifact(DATA_FILE, 'imitation_data')\n",
    "    \n",
    "    # call preprocessing function\n",
    "    do_preproc_imitation = ex.action(preproc_imitation, [imitation_data])\n",
    "    procd_imitation_data = ex.artifact('imitation_data.json', 'procd_imitation_data', do_preproc_imitation)\n",
    "    \n",
    "    # train the model \n",
    "    do_train_imitation_model = ex.action(train_imitation_model, [procd_imitation_data])\n",
    "    model = ex.artifact('model.pkl', 'model', do_train_imitation_model)\n",
    "    \n",
    "    model_tag = ex.literal(name='model_tag', v=model_tag)\n",
    "    \n",
    "    # serialize the wave entity, so we can track it as an artifact\n",
    "    serializeEntity(orgEntity, ENTITY_FILE)\n",
    "    granting_entity = ex.artifact(ENTITY_FILE, 'granting_entity')\n",
    "    \n",
    "    do_encrypt_model = ex.action(encrypt_model, [granting_entity, model, model_tag])\n",
    "    encrypted_model = ex.artifact('encrypted_model.bin', 'encrypted_model', do_encrypt_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encrypted_model.pull(utag=model_tag)\n",
    "model_location = encrypted_model.resolve_location()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make logging work correctly in the Jupyter notebook and set up Clipper\n",
    "import logging\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "from clipper_admin import DockerContainerManager, ClipperConnection\n",
    "from clipper_admin.deployers import python as py_deployer\n",
    "from clipper_util.auth_deployer import auth_deploy_python_model\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "clipper_conn = ClipperConnection(DockerContainerManager())\n",
    "clipper_conn.stop_all()\n",
    "clipper_conn.start_clipper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"pong-policy\"\n",
    "app_name = \"pong-\" + model_tag.v\n",
    "\n",
    "# load the encrypted model into memory\n",
    "with open(model_location, 'rb') as f:\n",
    "    ciphered_model = f.read()\n",
    "\n",
    "# \n",
    "auth_deploy_python_model(\n",
    "    clipper_conn,\n",
    "    model_name,\n",
    "    predict,\n",
    "    wave,\n",
    "    recipientEntity,\n",
    "    ciphered_model,    \n",
    "    version=1,\n",
    "    input_type=\"doubles\"\n",
    ")\n",
    "\n",
    "clipper_conn.register_application(name=app_name, default_output=\"0\", input_type=\"doubles\", slo_micros=100000)\n",
    "clipper_conn.link_model_to_app(app_name=app_name, model_name=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clipper_addr = clipper_conn.get_query_addr()\n",
    "\n",
    "import subprocess32 as subprocess\n",
    "server_handle = subprocess.Popen([\"./start_webserver.sh\", clipper_addr], stdout=subprocess.PIPE)\n",
    "print(server_handle.stdout.readline().strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@flor.func\n",
    "def start_ray(**kwargs):\n",
    "    try:\n",
    "        ray.get([])\n",
    "    except:\n",
    "        ray.init()    \n",
    "    return {'exit_code': 0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate an agent that can be trained using Proximal Policy Optimization (PPO)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the `PPOAgent` for some number of iterations.\n",
    "\n",
    "**EXERCISE:** You will need to experiment with the number of iterations as well as with the configuration to get the agent to learn something reasonable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checkpoint the agent so that the relevant model can be saved and deployed to Clipper. We save the name of the checkpoint file in `metadata.json` so the model container knows how to restore the policy checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@flor.func\n",
    "def train_agent(env_config, num_iterations, **kwargs):\n",
    "    register_env(\"my_env\", lambda ec: pong_py.PongJSEnv())\n",
    "    agent = ppo.PPOAgent(env=\"my_env\", config={\"env_config\": {}})\n",
    "    for i in range(num_iterations):\n",
    "        result = agent.train()\n",
    "    checkpoint_path = agent.save()\n",
    "    return {'checkpoint_path': checkpoint_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with flor.Experiment('rl-pong') as ex:\n",
    "    # make sure that Ray is running before attempting to train a model\n",
    "    do_start_ray = ex.action(start_ray, [])\n",
    "    exit_code = ex.literal(name='exit_code', parent=do_start_ray)\n",
    "    \n",
    "    # define configurations variables relevant to training the RL model\n",
    "    env_config = ex.literal({}, 'env_config') # TODO: Fill env_config\n",
    "    num_iterations = ex.literal(2, 'num_iterations')\n",
    "\n",
    "    # setup the training action and the save location of the checkpoint\n",
    "    do_train_agent = ex.action(train_agent, [env_config, num_iterations, exit_code])\n",
    "    checkpoint = ex.literal(name='checkpoint_path', parent=do_train_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint.pull()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
