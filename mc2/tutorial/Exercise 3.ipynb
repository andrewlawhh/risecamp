{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiparty XGBoost with Federated Training\n",
    "We will now discuss running XGBoost in the federated setting. Unlike the previous exercise, in the federated setting all data stays on its respective machine. This eliminates the need to transfer over the network which incurs high overhead and requires significant bandwidth. Instead, in the federated setting in each iteration each party sends a summary of the update made to its model. The central server then aggregates these updates, applies the aggregated update to its model, and broadcasts the new model to all parties. The parties then train locally with the new model and sends the update to the central server.\n",
    "\n",
    "![title](img/exercise3.png)\n",
    "\n",
    "In our project, all this is abstracted away. The central server simply starts the training, and everything else is performed automatically.\n",
    "\n",
    "Import some helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import subprocess\n",
    "from start_job import start_job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Edit hosts.config \n",
    "The `hosts.config` file should contain the IPs and ports of all workers in the federation. After loading in the `hosts.config` file, modify it to contain the IPs of all parties in the federation! Then write the new addresses back to the file by adding a magic to the top of the cell:\n",
    "\n",
    "`%%writefile hosts.config`\n",
    "\n",
    "Make sure to delete the `# %load hosts.config` line from the cell before saving it. We'll be continually using the `%load` and `%%writefile` magics in this tutorial to edit files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load hosts.config\n",
    "35.167.132.178:22\n",
    "34.222.205.126:22\n",
    "34.222.177.218:22"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Variables For Network Analysis\n",
    "We'll walk you through inspecting packets during this tutorial as well to make sure that the network topology is indeed federated. For each variable below, fill in the corresponding IP (don't worry about the ordering of the worker nodes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "master = '0'\n",
    "worker_1 = '1'\n",
    "worker_2 = '2'\n",
    "worker_3 = '3'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modifying the Training Script\n",
    "We will now modify the script that will be run for federated training. Load it in by running the following cell. The contents of the script should appear in the cell. \n",
    "\n",
    "The central server controls the training. If you're the central server, you can play with the `params` argument passed into the `train()` function. A list of possible parameters and their descriptions can be found [here](https://xgboost.readthedocs.io/en/latest/parameter.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load train_model.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using tcpdump to Capture Packets\n",
    "Here, we will be using `tcpdump` to monitor the network traffic during training. The cell below spawns a subprocess that records all incoming network traffic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tcpdump_cmd = 'tcpdump -ni en0 -s0 -w capture.pcap'\n",
    "tcpdump_process = subprocess.Popen(tcpdump_cmd, stdout=subprocess.PIPE, shell=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Job\n",
    "After modifying the script, we can start our job! We can use the `start_job()` helper function to do so.\n",
    "`start_job(num_parties, memory, script_path)` takes in three parameters:\n",
    "* num_parties: The number of parties in the federation. This should be the same as the number of IPs added to hosts.config\n",
    "* memory: The amount of memory to use for this job on each party's machine\n",
    "* script_path: The absolute path to the script we want to run\n",
    "\n",
    "After `start_job()` is finished, we want to kill the tcpdump subprocess in order to save the `.pcap` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_job(2, 3, \"/home/$USER/train_model.py\")\n",
    "tcpdump_process.terminate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Analysis\n",
    "The goal of this section is to show that the workers do not communicate with each other at all during training. First, let's convert the `pcap` file we created with `tcpdump` to a `.csv`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tshark -r capture.pcap -T fields -e frame.number -e eth.src -e eth.dst -e ip.src -e ip.dst -e frame.len -E header=y -E separator=, > capture.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading\n",
    "In the cells below, we first load in the `.csv` created by tshark into a pandas DataFrame and drop all rows that have `NaN` in either of the IP columns. Then, we rename the IPs corresponding to the members of the federation for easier reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "capture = pd.read_csv('capture.csv', names=['Frame Number', 'Ethernet Source', 'Ethernet Destination', \n",
    "                                            'IP Source', 'IP Destination', 'Frame Length'], header=0)\n",
    "capture.dropna(subset=['IP Source', 'IP Destination'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {master: 'Master', worker_1: 'worker_1', worker_2: 'worker_2', worker_3: 'worker_3'}\n",
    "capture.replace(labels, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "Create a new column in the table to see the communications to and from this particular node. In order to get the total number of bytes transmitted for each type of transmission, group by this new column and then sum all the frame lengths. In order to get the total number of packets sent by each transmission, simply count how many times that particular transmission occurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "capture['Transmission'] = capture.apply(lambda row: row['IP Source'] + ' -> ' + row['IP Destination'], axis=1)\n",
    "count_bytes = capture.groupby('Transmission', as_index=False)['Transmission', 'Frame Length'].sum()\n",
    "count_bytes.rename(mapper={'Frame Length': 'Total Bytes Transmitted'}, inplace=True, axis=1)\n",
    "count_packets = capture['Transmission'].value_counts().rename_axis('Transmission').reset_index(name='Number of Packets')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "In the cell below, we perform a join on the two tables, sort by the total number of bytes transmitted, and show the total communcations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_bytes.set_index('Transmission', inplace=True)\n",
    "count_packets.set_index('Transmission', inplace=True)\n",
    "counts = count_packets.join(count_bytes, on='Transmission')\n",
    "counts.sort_values(by='Total Bytes Transmitted', inplace=True, ascending=False)\n",
    "counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modifying the Evaluation Script\n",
    "We'll now use the model we trained in the previous step to make predictions on our test data. Load in the test script like in the previous step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load test_model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_job(2, 3, \"/home/$USER/test_model.py\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
